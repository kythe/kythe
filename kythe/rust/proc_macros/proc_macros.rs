//! Support for loading and interpreting a static set of procedural macros.
//!
//! To analyze source that uses macros, we must be able to expand them. For procedural macros,
//! this means executing them as native code. Proc macros are always built as dynamic libraries,
//! loaded with `dlopen()`, and speak a very specific calling convention/protocol defined by
//! `proc_macro::bridge` in the standard library.
//! https://fasterthanli.me/articles/proc-macro-support-in-rust-analyzer-for-nightly-rustc-versions
//!
//! We can compare our scheme with rustc and rust-analyzer, which also load proc macros:
//!
//!  - We load the dylibs into our main process, as rustc does.
//!
//!    rust-analyzer runs a separate process: rust-analyzer-proc-macro-srv.
//!    This isolates it from crashes and from certain version headaches.
//!    We manage these by controlling the set of macros we load at build time.
//!
//!  - The `proc_macro::bridge` protocol allows each embedder to provide its own data types.
//!    rust-analyzer and rustc have separate implementations using different data types.
//!
//!    We reuse rust-analyzer's types, and the `proc-macro-srv` crate which handles direct
//!    interactions with `proc_macro::bridge`.
//!    (It also provides convenient APIs for opening and interacting with dylibs).
//!
//!  - Both rustc and rust-analyzer consume dylibs built along with the examined source code -
//!    this ensures they can handle all proc macros that are used.
//!
//!    Here, we instead consume a fixed set of dylibs that are built along with the indexer.
//!    We may be missing some macros that the code uses, or suffer version skew: this will lead
//!    to an inaccurate/incomplete model of the code.
//!
//!  - Proc macro dylibs and host binary must ensure ABI-compatibility by building against the
//!    same standard library revision. [bridge_abi]
//!
//!    rustc achieves this by coupling rustc+stdlib together as the toolchain, and using that
//!    toolchain to build all proc macro dylibs.
//!    Similarly, rust-analyzer ships the proc-macro-srv binary with the toolchain.
//!
//!    We instead build the host binary and the dylibs at the same time, and ship them together.
//!
//! Limiting the set of supported macros is the main downside of this approach. But adding more to
//! the set is relatively easy, and go/rust-style discourages adding new proc macros.
//!
//! The benefits include decoupling from the toolchain, and having a simple security model where
//! all the code we're executing is expressed as build dependencies.
//!
//! ---
//!
//! [bridge_abi] The specific ABI requirements are imposed by proc_macro::bridge, and are unusual:
//!
//!   - the compiler versions need not match, as all interfaces are `extern "C"`
//!   - the proc_macro::bridge APIs are fairly stable at a source level
//!   - the concrete binary interface between client and server is generated by macros, and these
//!     are an unstable private implementation detail.
//!
//! This is why matching the standard library source revision is necessary and sufficient.

// Deps:
//   rust_analyzer:proc_macro_srv
//   crates.io:log
//   absl:status

use std::{
    any::Any,
    collections::HashMap,
    fmt::Debug,
    fs,
    path::{Path, PathBuf},
    sync::{Arc, LazyLock, Mutex},
};

use proc_macro_srv::{EnvSnapshot, ProcMacroKind, ProcMacroSrv};
use rust_analyzer::{
    base_db::Env,
    hir::{tt, Symbol},
    hir_expand::proc_macro::{
        ProcMacro, ProcMacroExpander, ProcMacroExpansionError, ProcMacroKind as HirProcMacroKind,
    },
    paths::Utf8PathBuf,
    span::Span,
};
use status_rs::StatusOr;

/// Loads a set of named proc macro dylibs from a directory on disk.
///
/// The libraries to be loaded should be listed in $SODIR/manifest, one-per-line:
///
///     foo=libfoo-123.so
///     bar=libbar-123.so
///
/// The names "foo" and "bar" will be the keys of the returned map.
/// In practice, clients use the bazel `rust_proc_macro` target label as the key.
///
/// Each proc-macro crate can export multiple named macros, each one is a ProcMacro object.
/// ProcMacro implements rust-analyzer's macro-expansion API, when `expand()` is called the request
/// is translated to the `proc_macro`-based API the dylib provides (via `proc_macro::bridge`).
pub fn load(sodir: &Path) -> StatusOr<HashMap<String, Vec<ProcMacro>>> {
    let srv = ProcMacroSrv::new(&ENV_SNAPSHOT);
    // The server is shared between the returned ProcMacros.
    // Expanding requires exclusive access.
    let srv = Arc::new(Mutex::new(srv));

    let manifest_path = sodir.join("manifest");
    log::info!("reading proc macros from {}", manifest_path.display());
    fs::read_to_string(manifest_path)
        .map_err(|e| status::invalid_argument(e.to_string()))?
        .lines()
        .map(|line| {
            let (key, dylib) = line
                .split_once('=')
                .ok_or_else(|| status::failed_precondition("malformed manifest.txt"))?;
            let dylib = Utf8PathBuf::from_path_buf(sodir.join(dylib))
                .map_err(|_| status::failed_precondition("bad path"))?;
            log::info!("loading proc macros for {key} from {dylib}");
            let macro_info =
                srv.lock().unwrap().list_macros(&dylib).map_err(|e| status::unknown(e))?;
            let macros = macro_info
                .into_iter()
                .map(|(name, kind)| {
                    log::info!(" ==> {name}");
                    ProcMacro {
                        name: Symbol::intern(&name),
                        kind: match kind {
                            ProcMacroKind::CustomDerive => HirProcMacroKind::CustomDerive,
                            ProcMacroKind::Attr => HirProcMacroKind::Attr,
                            ProcMacroKind::Bang => HirProcMacroKind::Bang,
                        },
                        expander: Arc::new(Expander {
                            name,
                            path: dylib.clone().into(),
                            srv: srv.clone(),
                        }),
                        disabled: false,
                    }
                })
                .collect();

            Ok((key.to_owned(), macros))
        })
        .collect()
}

// Ideally we'd use an empty environment, but there's no such constructor.
static ENV_SNAPSHOT: LazyLock<EnvSnapshot> = LazyLock::new(Default::default);

/// Adapts a ProcMacroSrv to expand a specific macro (ProcMacroExpander trait).
struct Expander {
    /// The server, which already has all dylibs loaded.
    /// This is shared between expanders of all macros in all proc-macro crates.
    srv: Arc<Mutex<ProcMacroSrv<'static>>>,
    /// The path of the proc-macro crate dylib.
    /// The dylib was already loaded, so this is merely a key at this point.
    path: PathBuf,
    /// The name of the specific macro within the proc-macro crate.
    name: String,
}

impl Debug for Expander {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.write_str(&format!("[{} from {}]", &self.name, self.path.display()))
    }
}

impl PartialEq for Expander {
    fn eq(&self, other: &Self) -> bool {
        Arc::ptr_eq(&self.srv, &other.srv) && self.path == other.path && self.name == other.name
    }
}

/// Handles a macro expansion by delegating to the loaded dylib via ProcMacroSrv.
///
/// In rust-analyzer, this is split across:
///  - the client: `proc_macro_api::ProcMacro::expand()`, running in the host
///  - the server: `proc_macro_srv::ProcMacroSrv::expand`, in the proc-macro-srv subprocess.
///
/// The sequence is:
///
/// 1. client sets up "flat" token-tree data structures (good for serialization)
/// 2. it sends them to the `proc-macro-srv` subprocess, serialized as JSON
/// 3. the server unflattens them into regular token-trees
/// 4. the server calls srv.expand()
///    (this is the part that talks to the dylib via proc_macro::bridge)
/// 5. the result is flattened
/// 6. this tree is sent back to the client
/// 7. client unpacks this tree into the regular token-tree structures
///
/// We skip the serialization/flatten/subprocess step and call srv.expand().
///
/// We don't need to care about version negotiation for different token-tree
/// representations, we can always use the latest one.
impl ProcMacroExpander for Expander {
    fn expand(
        &self,
        subtree: &tt::TopSubtree,
        attrs: Option<&tt::TopSubtree>,
        env: &Env,
        def_site: Span,
        call_site: Span,
        mixed_site: Span,
        current_dir: String,
    ) -> Result<tt::TopSubtree, ProcMacroExpansionError> {
        let trees = self
            .srv
            .lock()
            .unwrap()
            .expand(
                Utf8PathBuf::try_from(self.path.clone()).unwrap(),
                env.clone().into(),
                Some(current_dir),
                self.name.to_string(),
                subtree.clone(),
                attrs.cloned(),
                def_site,
                call_site,
                mixed_site,
            )
            .map_err(ProcMacroExpansionError::Panic)?;
        Ok(rust_analyzer::tt::TopSubtree(trees.into_boxed_slice()))
    }

    fn eq_dyn(&self, other: &dyn ProcMacroExpander) -> bool {
        (other as &dyn Any).downcast_ref::<Self>().is_some_and(|other| self == other)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use googletest::prelude::*;
    use rust_analyzer::{
        hir::tt::{Leaf, TtElement},
        hir_expand::span_map::{RealSpanMap, SpanMap},
        ide::Edition,
        span::EditionedFileId,
        syntax::{self, ast, AstNode},
        syntax_bridge::{syntax_node_to_token_tree, DocCommentDesugarMode},
        tt::DelimiterKind,
        vfs::FileId,
    };
    use std::path::Path;

    #[gtest]
    fn expand_import() -> googletest::Result<()> {
        // Load the bundled proc macros, and make sure import::import is there.
        let macros =
            load(Path::new("proc_macros/standard_proc_macros"))?;
        let import_macros = macros
            .get("//import_macro:import_macro")
            .or_fail()?;
        let import_macro: &ProcMacro =
            import_macros.iter().find(|m| m.name.as_str() == "import").or_fail()?;
        expect_that!(import_macro.kind, eq(HirProcMacroKind::Bang));

        // Parse some code that we can feed into macro expansion.
        const CODE: &str = r#"
           use a;
           import::import!{"//package:target" as b;}
           use c;
        "#;
        let parsed = syntax::SourceFile::parse(CODE, Edition::CURRENT);
        let span_map = SpanMap::RealSpanMap(
            RealSpanMap::absolute(EditionedFileId::new(FileId::from_raw(0), Edition::CURRENT))
                .into(),
        );
        let macro_call =
            parsed.syntax_node().descendants().find_map(ast::MacroCall::cast).or_fail()?;
        let call_site = span_map.span_for_range(macro_call.syntax().text_range());
        let mut original = syntax_node_to_token_tree(
            macro_call.token_tree().or_fail()?.syntax(),
            &span_map,
            call_site,
            DocCommentDesugarMode::ProcMacro,
        );
        // The braces in import!{} are not part of the macro input.
        original.top_subtree_delimiter_mut().kind = DelimiterKind::Invisible;
        fn token(elt: TtElement) -> Option<&Leaf> {
            match elt {
                TtElement::Leaf(leaf) => Some(leaf),
                _ => None,
            }
        }
        let original_tokens: Vec<_> = original.iter().map(token).collect();

        // Expand the macro in question. (Normally the parser does this).
        let expanded = import_macro
            .expander
            .expand(
                &original,
                None,
                &Env::default(),
                call_site,
                call_site,
                call_site,
                String::new(),
            )
            .or_fail()?;

        let expanded_delim = expanded.top_subtree().delimiter;
        expect_eq!(expanded_delim.kind, DelimiterKind::Invisible);

        // Check the token sequence.
        // "//package:target" as b; ==> extern crate xxx as b;
        let expanded_tokens: Vec<_> = expanded.iter().map(token).collect();
        let expanded_text: Vec<_> =
            expanded_tokens.iter().map(|x| x.as_ref().map(ToString::to_string)).collect();
        expect_that!(
            expanded_text,
            elements_are!(
                some(eq("extern")),
                some(eq("crate")),
                some(anything()),
                some(eq("as")),
                some(eq("b")),
                some(eq(";"))
            )
        );

        // The spans track the relationship between the written and expanded tokens.

        // The macro ties the mangled crate name to the label literal from the input.
        // (This test is a little brittle: other choices would also be reasonable).
        let original_label = &original_tokens[0];
        let expanded_xxx = &expanded_tokens[2];
        expect_eq!(expanded_xxx.unwrap().span(), original_label.unwrap().span());

        // The provenance of the b token should be exactly preserved.
        let original_b = &original_tokens[2];
        let expanded_b = &expanded_tokens[4];
        expect_eq!(expanded_b.unwrap().span(), original_b.unwrap().span());

        // The output tokens form an invisible delimited group.
        // Its bounds should match the whole macro invocation.
        let original_range = span_map.span_for_range(macro_call.syntax().text_range());
        let expanded_range = expanded_delim.open.cover(expanded_delim.close);
        expect_eq!(expanded_range, original_range);

        Ok(())
    }
}
